\title{Graduate School Pre-exam Solution}
\author{Daeyoung Lim}

\documentclass[answers]{exam}
\usepackage[left=3cm,right=3cm,top=3.5cm,bottom=2cm]{geometry}
\usepackage{amssymb,amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{kotex}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
% \usepackage{enumerate}
\usepackage{listings}
\usepackage{courier}
\usepackage{cancel}
\usepackage{array}
\usepackage{courier}
\usepackage{booktabs}
\usepackage{titlesec}
\usepackage[shortlabels]{enumitem}
\usepackage{setspace}
\usepackage{empheq}
\usepackage{tikz}
\usepackage{listings}

% \usepackage[toc,page]{appendix}

\setlength{\heavyrulewidth}{1.5pt}
\setlength{\abovetopsep}{4pt}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\newcommand\encircle[1]{%
  \tikz[baseline=(X.base)] 
    \node (X) [draw, shape=circle, inner sep=0] {\strut #1};}
 
% Command "alignedbox{}{}" for a box within an align environment
% Source: http://www.latex-community.org/forum/viewtopic.php?f=46&t=8144
\newlength\dlf  % Define a new measure, dlf
\newcommand\alignedbox[2]{
% Argument #1 = before & if there were no box (lhs)
% Argument #2 = after & if there were no box (rhs)
&  % Alignment sign of the line
{
\settowidth\dlf{$\displaystyle #1$}  
    % The width of \dlf is the width of the lhs, with a displaystyle font
\addtolength\dlf{\fboxsep+\fboxrule}  
    % Add to it the distance to the box, and the width of the line of the box     ㅊ
\hspace{-\dlf}  
    % Move everything dlf units to the left, so that & #1 #2 is aligned under #1 & #2
\boxed{#1 #2}
    % Put a box around lhs and rhs
}
}
\setcounter{secnumdepth}{4}
\lstset{
         basicstyle=\footnotesize\ttfamily, % Standardschrift
         %numbers=left,               % Ort der Zeilennummern
         numberstyle=\tiny,          % Stil der Zeilennummern
         %stepnumber=2,               % Abstand zwischen den Zeilennummern
         numbersep=5pt,              % Abstand der Nummern zum Text
         tabsize=2,                  % Groesse von Tabs
         extendedchars=true,         %
         breaklines=true,            % Zeilen werden Umgebrochen
         keywordstyle=\color{red},
            frame=b,         
 %        keywordstyle=[1]\textbf,    % Stil der Keywords
 %        keywordstyle=[2]\textbf,    %
 %        keywordstyle=[3]\textbf,    %
 %        keywordstyle=[4]\textbf,   \sqrt{\sqrt{}} %
         stringstyle=\color{white}\ttfamily, % Farbe der String
         showspaces=false,           % Leerzeichen anzeigen ?
         showtabs=false,             % Tabs anzeigen ?
         xleftmargin=17pt,
         framexleftmargin=17pt,
         framexrightmargin=5pt,
         framexbottommargin=4pt,
         %backgroundcolor=\color{lightgray},
         showstringspaces=false      % Leerzeichen in Strings anzeigen ?        
 }
 \lstloadlanguages{% Check Dokumentation for further languages ...
         %[Visual]Basic
         %Pascal
         %C
         %C++
         %XML
         %HTML
         Java
 }
    %\DeclareCaptionFont{blue}{\color{blue}} 

\definecolor{myblue}{RGB}{72, 165, 226}
\definecolor{myorange}{RGB}{222, 141, 8}
\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\setlength{\heavyrulewidth}{1.5pt}
\setlength{\abovetopsep}{4pt}
\setlength{\parindent}{0mm}
\linespread{1.3}
\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\csch}{csch}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator{\Tr}{Tr}

\newcommand{\bs}{\boldsymbol}
\newcommand{\opn}{\operatorname}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % We use newtheorem to define theorem-like structures
% %
% % Here are some common ones. . .
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{scolium}{Scolium}   %% And a not so common one.
\newtheorem{definition}{Definition}
\newenvironment{proof}{{\sc Proof:}}{~\hfill QED}
\newenvironment{AMS}{}{}
\newenvironment{keywords}{}{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %   The first thanks indicates your affiliation
% %
% %  Just the name here.
% %
% % Your mailing address goes at the end.
% %
% % \thanks is also how you indicate grant support
% %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
\setstretch{1.5} %줄간격 조정
\newpage
\firstpageheader{}{}{\bf\large Daeyoung Lim \\ Grad School \\ Year of 2013, early}
\runningheader{Daeyoung Lim}{Graduate School Pre-exam}{2013 early}
\begin{questions}
   \question
    (25점) Suppose that $X$ and $Y$ have the following joint probability density function
    \begin{equation}
      f_{X,Y}\left(x,y\right) = \begin{cases}\dfrac{1}{2}\left(x+y\right)e^{-\left(x+y\right)},& x,y\geq 0\\0, & \text{otherwise}  \end{cases}
    \end{equation}
    \begin{enumerate}[(a)]
      \item Find the moment generating function of $Z=X+Y$.
      \item Find the expected value of $W=X/\left(X+Y\right)$.
      \item Are $Z$ and $W$ independent?
    \end{enumerate}
    \begin{solution}
      \begin{enumerate}[(a)]
        \item $Z=X+Y$, $X=X$로 놓고 변수변환을 하게 되면
        \begin{align}
          J&=\begin{bmatrix}1 & 0 \\ -1 & 1\end{bmatrix}\\
          \left|J\right| &= 1\\
          f_{X,Z}\left(x,z\right) &= \dfrac{1}{2}ze^{-z},\quad 0\leq x \leq z
        \end{align}
        따라서 적분해서 $x$를 소거하면
        \begin{equation}
          f_{Z}\left(z\right) = \int_{0}^{z}\dfrac{1}{2}ze^{-z}\,dx = \dfrac{1}{2}z^{2}e^{-z}
        \end{equation}
        이 되어 $Z\sim\mathrm{Ga}\left(3,1\right)$임을 알 수 있다. MGF를 직접 정의를 이용해 구해도 상관없지만 이미 꼴을 알고 있다면 그냥 감마분포임을 이용해 구해도 무방하다. 정의를 통해 구해보자.
        \begin{align}
          \mathrm{E}\left(e^{tZ}\right) &= \int_{0}^{\infty}\dfrac{1}{2}z^{2}e^{-\left(1-t\right)z}\,dz\\
          &=\dfrac{1}{\cancel{2}}\dfrac{\cancel{\Gamma\left(3\right)}}{\left(1-t\right)^{3}}\\
          &=\left(1-t\right)^{-3}
        \end{align}
        \item \emph{The law of unconscious statistician}에 따라 다음과 같이 구할 수 있다.
        \begin{align}
          \mathrm{E}\left(\dfrac{X}{X+Y}\right) &= \int_{0}^{\infty}\int_{0}^{\infty}\dfrac{x}{x+y}\dfrac{1}{2}\left(x+y\right)e^{-\left(x+y\right)}\,dx\,dy\\
          &=\int_{0}^{\infty}\dfrac{1}{2}e^{-y}\underbrace{\int_{0}^{\infty}xe^{-x}\,dx}_{=1}\,dy\\
          &=\dfrac{1}{2}\underbrace{\int_{0}^{\infty}e^{-y}\,dy}_{=1}\\
          &=\dfrac{1}{2}
        \end{align}
        \item 둘의 결합밀도함수를 구해보자. $Z=X+Y$, $W=X/\left(X+Y\right)$이므로
        \begin{align}
          \left|J\right| &= z\\
          f_{W,Z}\left(w,z\right) &= f_{X,Y}\left(wz,\left(1-w\right)z\right)\cdot z\\
          &=\dfrac{1}{2}z^{2}e^{-z}
        \end{align}
        범위가 중요한데 $x=wz$, $y=z-wz$이고 $x>0,y>0$이므로 $wz>0, z>wz$가 되어 $z>0,1>w>0$이 나온다. 즉 $W,Z$는 서로 의존하지 않고 결합밀도함수 역시 주변밀도함수의 곱으로 이뤄짐을 알 수 있다. $Z\sim\mathrm{Ga}\left(3,1\right),\; W\sim\mathrm{Unif}\left(0,1\right)$이다. 이는 (b)에서 구했던 $W$의 기댓값과도 일치한다. 따라서 $W,Z$는 서로 독립이다.
      \end{enumerate}
    \end{solution}
    \question
    (25점) $X_{1},X_{2},\ldots,X_{n}$을 평균이 $\mu$이고 분산이 $\sigma^{2}(<\infty)$인 i.i.d한 표본이라고 할 때
    \begin{enumerate}[(a)]
      \item 모평균 $\mu$에 대한 적률추정치(moment estimator)와 최소제곱추정치(least squared estimator) 모두가 표본 평균임을 보이시오.
      \item 표본평균이 일치추정량(consistent estimator)임을 보이시오.
    \end{enumerate}
    \begin{solution}
      \begin{enumerate}[(a)]
        \item 모평균의 적률추정치가 1차 표본적률(표본평균)이 되는 것은 당연하다. 최소제곱추정치는 대표값으로 `평균'을 사용하는 것을 정당화해준다. 다시 말해, 손실함수(loss function)을 오차의 제곱합으로 잡을 경우 그것의 최소화하는 것은 평균이 된다. 먼저 표본이 아닌 모집단으로 생각해보자.
        \begin{equation}
          \hat{c} = \argmin_{c}\mathrm{E}\left(\left(X-c\right)^{2}\right)
        \end{equation}
        MSE(mean squared error)의 관점에서 가장 좋은 추정량은 $\hat{c}=\mathrm{E}\left(X\right)$이 된다.\par
        \begin{proof}
          \begin{align}
            \mathrm{E}\left(\left(X-\mathrm{E}\left(X\right)+\mathrm{E}\left(X\right)-c\right)^{2}\right)&=\mathrm{E}\left[\left(X-\mathrm{E}\left(X\right)\right)^{2}\right.\\
            &\quad \left.+2\cancel{\left(X-\mathrm{E}\left(X\right)\right)\left(\mathrm{E}\left(X\right)-c\right)}+\left(\mathrm{E}\left(X\right)-c\right)^{2} \right]\\
            &=\mathrm{E}\left[\left(X-\mathrm{E}\left(X\right)\right)^{2}+\left(\mathrm{E}\left(X\right)-c\right)^{2} \right]\\
            \hat{c} &= \mathrm{E}\left(X\right)
          \end{align}
        \end{proof}\par
        다시 표본의 입장에서 미분을 통해 증명하면\par
        \begin{proof}
          \begin{align}
          f\left(c\right) &= \sum_{i=1}^{n}\left(X_{i}-c\right)^{2}\\
          &=\sum_{i=1}^{n}X_{i}^{2}-2c\sum_{i=1}^{n}X_{i}+nc^{2}\\
          \dfrac{d}{dc}f\left(c\right) &= -2\sum_{i=1}^{n}X_{i}+2nc=0\\
          \hat{c}&=\dfrac{1}{n}\sum_{i=1}^{n}X_{i}
          \end{align}
        \end{proof} \par
        따라서
        \begin{equation}
          \sum_{i=1}^{n}\left(X_{i}-c^{\text{sam}}\right)^{2}\xrightarrow{p}\mathrm{E}\left(\left(X-c^{\text{pop}}\right)^{2}\right)
        \end{equation}
        이므로 $\hat{c}^{\text{sam}}\xrightarrow{p}\hat{c}^{\text{pop}}$이다. 즉, 평균의 최소제곱추정량이 $\hat{c}^{\text{sam}}$이 되어 표본평균이 된다.
        \item 약대수의 법칙(\emph{the weak law of large numbers})를 증명하는 것인데 이는 \emph{Chebyshev's inequality}를 이용하면 쉽다.
        \begin{lemma}
          (Chebyshev's inequality) Suppose $0\leq t < \left\|f\right\|_{\infty}$. Define
          $$
            A=\left\{x\in X\,|\,\left|f\left(x\right)\right|\geq t \right\}.
          $$
          Then,
          $$
            \mu\left(A\right)\leq \left(\dfrac{\left\|f\right\|_{p}}{t}\right)^{p}.
          $$
        \end{lemma}
        \begin{proof}
          $L^{\infty}$ norm의 정의에 따라 $\mu\left(A\right)>0$로 $A$는 $\mu$-null set이 아니다. 따라서
          \begin{align}
            \left\|f\right\|_{p}&\geq \left(\int_{A}\left|f\right|^{p}\,d\mu\right)^{1/p}\\
            &\geq \left(t^{p}\mu\left(A\right)\right)^{1/p}\\
            &= t\mu\left(A\right)^{1/p}
          \end{align}
        \end{proof}\par
        따라서 확률론적으로 우리가 아는 평균에 대한 부등식으로 표현하면 다음과 같아진다.
        \begin{equation}
          \mathrm{Pr}\left(\left|\overline{X}_{n}-\mathrm{E}\left(X\right)\right|\geq \lambda\right)\leq \dfrac{\mathrm{E}\left|X-\mathrm{E}\left(X\right)\right|^{2}}{n\lambda^{2}}
        \end{equation}
        원래는 부등식의 우변에 $\overline{X}_{n}$에 대한 식이 와야 하는데 다음의 항등식으로 인해 위와 같이 쓸 수 있다.
        $$
          \mathrm{E}\left|\overline{X}_{n}-\mathrm{E}\left(X\right)\right|^{2}= \dfrac{1}{n}\mathrm{E}\left|X-\mathrm{E}\left(X\right)\right|^{2},\;\left(\mathrm{Var}\left(\overline{X}_{n}\right)=\dfrac{1}{n}\mathrm{Var}\left(X\right)\right)
        $$
        따라서 $n\to\infty$함에 따라
        \begin{equation}
          \lim_{n\to\infty}\mathrm{Pr}\left(\left|\overline{X}_{n}-\mathrm{E}\left(X\right)\right|\geq \lambda\right)=0
        \end{equation}
        고로 $\overline{X}_{n}$는 $\mathrm{E}\left(X\right)$에 대한 일치추정량이다. 그냥 \emph{Chebyshev's inequality} 증명 없이 바로 써도 상관없다.
      \end{enumerate}
    \end{solution}
    \question
    (25점) We want to develop general methods for testing hypotheses and apply those methods to some common problems.
    \begin{enumerate}[(a)]
      \item Briefly explain what a uniformly most powerful (UMP) test is. In particular, what is the meaning of ``uniformly''?
      \item Let $X_{1},\ldots,X_{n}$ be a random sample from $f\left(x\right)=\theta e^{-\theta x}I_{\left(0,\infty\right)}\left(x\right)$. Find a UMP test of $H_{0},\theta=\theta_{0}$ versus $H_{1}:\theta>\theta_{0}$.
    \end{enumerate}
    \begin{solution}
      \begin{enumerate}[(a)]
        \item (\emph{Casella-Berger}에 따르면) $\mathcal{C}$이 어떤 모수 $\theta$에 대해 다음을 두 가설에 대한 검정들의 집합이라 하자.
        \begin{equation}
          H_{0}:\theta\in\Theta_{0}\quad \text{versus} \quad H_{1}:\theta\in\Theta_{0}^{\mathsf{c}}
        \end{equation}
        만약 $\mathcal{C}$에 속하는 어떤 검정의 검정력함수 $\beta\left(\theta\right)$이 그 어떤 $\theta\in\Theta_{0}^{\mathsf{c}}$와 그 어떤 $\beta'\left(\theta\right)$을 택해도 $\beta\left(\theta\right)\geq \beta'\left(\theta\right)$일 때 우리는 그 검정을 균일최강력검정(UMP test)라 부른다.\par
        우리가 평소에 자주 쓰는 가능도비는 균일최강력검정을 유도하기 위한 방법으로 그 정당성은 \emph{Neyman-Pearson lemma}(simple hypotheses), \emph{Karlin-Rubin theorem}(composite hypotheses)에 있다. 여기서 `균일(uniform)'이란 말이 붙은 것은 원래 simple alternative hypothesis를 어떻게 잡아도 최강력검정이 되기 때문에 `균일하게' 다 발라버린다는 의미로 쓰인 것인데, simple vs simple일 때도 대립가설의 모수공간이 singleton이기 때문에 다른 simple을 잡을 수 없을 뿐이지 균일하게 최강력이긴 하다. 따라서 구분하지 않아도 무방하지만 그래도 simple hypotheses일 때는 `균일'하다는 말을 붙이는 것이 무의미하긴 하다.
        \item \emph{Karlin-Rubin theorem}에 따라 가능도비를 구하고 \emph{monotone likelihood ratio}가 되는지를 확인해야 한다. $\theta_{1}>\theta_{0}$인 대립가설의 모수 $\theta_{1}$ 하나를 임의로 택하자.
        \begin{align}
          \dfrac{L_{1}}{L_{0}} &= \left(\frac{\theta_{0}}{\theta_{1}}\right)^{n}\exp\left(\left(\theta_{1}-\theta_{0}\right)\sum_{i=1}^{n}X_{i}\right)\leq c_{1}\\
          \sum_{i=1}^{n}X_{i}&\leq c_{2}
        \end{align}
        따라서 기각역은 어떤 상수 $k$에 대해 $\displaystyle \sum_{i=1}^{n}X_{i}\leq k$일 때이다. 다음을 이용하여 $k$를 구한다.
        \begin{equation}
          2\theta\sum_{i=1}^{n}X_{i}\sim\chi^{2}\left(2n\right)
        \end{equation}
        그러므로
        \begin{equation}
          \mathrm{Pr}\left(2\theta\sum_{i=1}^{n}X_{i}\leq \chi_{2n}^{2}\left(1-\alpha\right)\,\middle|\,H_{0}\right)=\alpha
        \end{equation}
      \end{enumerate}
    \end{solution}
    \question
    (25점) Consider the regression model
    \begin{equation}
      \mathrm{E}\left(Y_{i}\right) = \beta_{0}-2\beta_{2}+\beta_{1}x_{i}+3\beta_{2}x_{i}^{2},\quad \mathrm{Var}\left(Y_{i}\right)=\sigma^{2}\;\;\text{for } i=1,2,3.
    \end{equation}
    Suppose that we observe $x_{1}=-1, x_{2}=0$, and $x_{3}=1$.
    \begin{enumerate}[(a)]
      \item Find the least square estimates of the parameters $\beta_{0},\beta_{1},$ and $\beta_{2}$.
      \item Find the covariance of the least squares estimate of $\beta_{1}$ and the least squares estimate of $\beta_{2}$.
    \end{enumerate}
    \begin{solution}
      \begin{enumerate}[(a)]
        \item 모형을 행렬표현으로 바꾸면
        \begin{equation}
          \underbrace{\begin{bmatrix}y_{1}\\y_{2}\\y_{3} \end{bmatrix}}_{Y} = \underbrace{\begin{bmatrix}1 & x_{1}& 3x_{1}^{2}-2\\ 1 & x_{2} & 3x_{2}^{2}-2 \\1 & x_{3} & 3x_{3}^{2}-2 \end{bmatrix}}_{\mathbf{X}}\underbrace{\begin{bmatrix}\beta_{0}\\ \beta_{1}\\\beta_{2} \end{bmatrix}}_{\beta}+\underbrace{\begin{bmatrix}\epsilon_{1}\\ \epsilon_{2}\\ \epsilon_{3} \end{bmatrix}}_{\epsilon}
        \end{equation}
        그리고 관측치가 $x_{1}=-1, x_{2}=0,x_{3}=1$이므로 계산이 단순해질 것을 기대하고 대입해보자.
        \begin{align}
          \mathbf{X}'\mathbf{X} &= \begin{bmatrix}1 & 1 &1\\ -1 & 0 & 1\\ 1 & -2 & 1 \end{bmatrix}\begin{bmatrix}1 & -1 & 1\\ 1 & 0 & -2 \\ 1 & 1 & 1 \end{bmatrix}\\
          &= \begin{bmatrix}3 & 0  & 0 \\ 0 & 2 & 0\\ 0 & 0 & 6 \end{bmatrix}\\
          \mathbf{X}'y &= \begin{bmatrix}y_{1}+y_{2}+y_{3}\\ y_{3}-y_{1}\\ y_{1}-2y_{2}+y_{3} \end{bmatrix}\\
          \widehat{\beta} &= \begin{bmatrix}\dfrac{1}{3}\left(y_{1}+y_{2}+y_{3}\right)\\\dfrac{1}{2}\left(y_{3}-y_{1}\right)\\\dfrac{1}{6}\left(y_{1}-2y_{2}+y_{3}\right) \end{bmatrix}
        \end{align}
        \item $Y_{i}$가 서로 독립이라는 것과 공분산 연산자의 bilinearity를 이용해서 전개해서 구해도 되겠지만, 일반적으로
        \begin{equation}
          \mathrm{Var}\left(\widehat{\beta}\right) = \sigma^{2}\left(\mathbf{X}'\mathbf{X}\right)^{-1}
        \end{equation}
        임을 이용하면 더 쉽다. 왜냐하면 대각원소를 제외한 상삼각행렬/하삼각행렬이 모두 0이기 때문에
        \begin{equation}
          \mathrm{Cov}\left(\widehat{\beta}_{i},\widehat{\beta}_{j}\right) =\begin{cases}\sigma^{2}\mathbf{C}_{(i+1)(i+1)}, & \text{if $i=j$}\\ 0, & \text{if $i\neq j$} \end{cases} \quad \text{for $i,j=0,1,2$}
        \end{equation}
        이고 여기서 $\mathbf{C}=\left(\mathbf{X}'\mathbf{X}\right)^{-1}$이다. 따라서 $\mathrm{Cov}\left(\widehat{\beta}_{1},\widehat{\beta}_{2}\right)=0$이다.
      \end{enumerate}
    \end{solution}
\end{questions}
\end{document}
