\title{Graduate School Pre-exam Solution}
\author{Daeyoung Lim}

\documentclass[answers]{exam}
\usepackage[left=3cm,right=3cm,top=3.5cm,bottom=2cm]{geometry}
\usepackage{amssymb,amsmath}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{courier}
\usepackage{cancel}
\usepackage{array}
\usepackage{courier}
\usepackage{booktabs}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage[usenames, dvipsnames]{color}
\setcounter{secnumdepth}{4}
\lstset{
         basicstyle=\footnotesize\ttfamily, % Standardschrift
         %numbers=left,               % Ort der Zeilennummern
         numberstyle=\tiny,          % Stil der Zeilennummern
         %stepnumber=2,               % Abstand zwischen den Zeilennummern
         numbersep=5pt,              % Abstand der Nummern zum Text
         tabsize=2,                  % Groesse von Tabs
         extendedchars=true,         %
         breaklines=true,            % Zeilen werden Umgebrochen
         keywordstyle=\color{red},
            frame=b,         
 %        keywordstyle=[1]\textbf,    % Stil der Keywords
 %        keywordstyle=[2]\textbf,    %
 %        keywordstyle=[3]\textbf,    %
 %        keywordstyle=[4]\textbf,   \sqrt{\sqrt{}} %
         stringstyle=\color{white}\ttfamily, % Farbe der String
         showspaces=false,           % Leerzeichen anzeigen ?
         showtabs=false,             % Tabs anzeigen ?
         xleftmargin=17pt,
         framexleftmargin=17pt,
         framexrightmargin=5pt,
         framexbottommargin=4pt,
         %backgroundcolor=\color{lightgray},
         showstringspaces=false      % Leerzeichen in Strings anzeigen ?        
 }
 \lstloadlanguages{% Check Dokumentation for further languages ...
         %[Visual]Basic
         %Pascal
         %C
         %C++
         %XML
         %HTML
         Java
 }
    %\DeclareCaptionFont{blue}{\color{blue}} 

\definecolor{myblue}{RGB}{72, 165, 226}
\definecolor{myorange}{RGB}{222, 141, 8}
\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\setlength{\heavyrulewidth}{1.5pt}
\setlength{\abovetopsep}{4pt}
\setlength{\parindent}{0mm}
\linespread{1.3}
\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\csch}{csch}
\DeclareMathOperator{\argmin}{\arg\!\min}
\DeclareMathOperator{\Tr}{Tr}

\newcommand{\bs}{\boldsymbol}
\newcommand{\opn}{\operatorname}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % We use newtheorem to define theorem-like structures
% %
% % Here are some common ones. . .
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{scolium}{Scolium}   %% And a not so common one.
\newtheorem{definition}{Definition}
\newenvironment{proof}{{\sc Proof:}}{~\hfill QED}
\newenvironment{AMS}{}{}
\newenvironment{keywords}{}{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %   The first thanks indicates your affiliation
% %
% %  Just the name here.
% %
% % Your mailing address goes at the end.
% %
% % \thanks is also how you indicate grant support
% %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
\newpage
\firstpageheader{}{}{\bf\large Daeyoung Lim \\ Grad School \\ Year of 2007, early}
\runningheader{Daeyoung Lim}{Graduate School Pre-exam}{2007 early}
\begin{questions}
   \question
   \textbf{(1)} $X_{1}, X_{2}, \ldots , X_{n}$ are a random sample whose p.m.f. is $\mathbb{P}\left(X = x\right) = \left(1-\theta\right)^{x}\theta, \; x = 0, 1, 2, \cdots$. What is the distribution of $\min \left(X_{1}, X_{2}, \ldots , X_{N}\right)$?\\
   \textbf{(2)} $X_{1}, X_{2}, \ldots , X_{n}$ are independent random variables, of which the distribution of $X_{i}$ is an exponential distribution whose mean is $\mu_{i}$ ($i = 1, 2, \ldots , n$). What is the distribution of $\min \left(X_{1}, X_{2}, \ldots , X_{n}\right)$?
\begin{solution}
   \textbf{(1)} We start with the c.d.f. of $\min \left(X_{1}, X_{2}, \ldots , X_{n}\right)$. For notational convenience, let us write $X_{\left(1\right)} = \min \left(X_{1}, X_{2}, \ldots , X_{n}\right)$.
   \begin{align*}
      \mathbb{P}\left(X_{\left(1\right)} \leq x\right) &= 1 - \mathbb{P}\left(X_{1}>x, X_{2}>x, \ldots, X_{n}>x\right)\\
      &= 1-\left(1-\sum_{i=0}^{x}\left(1-\theta\right)^{i}\theta \right)^{n}\\
      &= 1 - \left(1-\theta \frac{1-\left(1-\theta\right)^{x+1}}{1-\left(1-\theta\right)}\right)^{n}\\
      &= 1- \left\{1-\theta \right\}^{n\left(x+1\right)}
   \end{align*}
   Recall we are dealing with a discrete distribution. To get the p.m.f. from the c.d.f., we subtract the $\left(x-1\right)^{\text{th}}$ term from the $x^{\text{th}}$ term.
   \begin{align*}
      F\left(x\right) - F\left(x-1\right) &= 1- \left(1-\theta\right)^{n\left(x+1\right)} - 1 + \left(1-\theta\right)^{nx}\\
      &= \left(1-\theta\right)^{nx} - \left(1-\theta\right)^{n\left(x+1\right)}\\
      &= \left(1-\theta\right)^{nx}\left\{1 - \left(1-\theta\right)^{n} \right\}\\
      &\sim \opn{Geo}\left(1-\left(1-\theta\right)^{n})
   \end{align*}
   It turned out to be the geometric distribution whose random variable $X$ indicates the number of failures before observing the first success.
\end{solution}
\begin{solution}
   \textbf{(2)} The p.d.f. of the $i^{\text{th}}$ random variable is $f_{X_{i}}\left(x\right) = \frac{1}{\mu_{i}}e^{-x/\mu_{i}}$. Therefore,
   \begin{align*}
      \mathbb{P}\left(X_{\left(1\right)}\leq x\right) &= 1-\mathbb{P}\left(X_{1}>x\right)\mathbb{P}\left(X_{2}>x\right)\cdots \mathbb{P}\left(X_{n}>x\right)\\
      &= 1-\prod_{i=1}^{n}\int_{x}^{\infty}\frac{1}{\mu_{i}}e^{-x_{i}/\mu_{i}}\,dx_{i}\\
      &= 1-\prod_{i=1}^{n} e^{-x/\mu_{i}}\\
      &= 1-\exp\left\{-\left(\frac{x}{\mu_{1}}+\frac{x}{\mu_{2}} + \cdots + \frac{x}{\mu_{n}} \right)\right\}\\
      &= 1 - \exp\left\{-x\sum_{i=1}^{n}\frac{1}{\mu_{i}}\right\}
   \end{align*}
   Since the distribution is continuous, we can differentiate the c.d.f. to compute the p.d.f..
   \begin{align*}
      \frac{d}{dx}\mathbb{P}\left(X_{\left(1\right)}\leq x \right) &= f_{X_{\left(1\right)}}\left(x\right)\\
      &= \left(\sum_{i=1}^{n} \frac{1}{\mu_{i}}\right) \exp \left\{-x\sum_{i=1}^{n}\frac{1}{\mu_{i}} \right\}\\
      &\sim \opn{Exp}\left(\sum_{i=1}^{n}\frac{1}{\mu_{i}} \right)
   \end{align*}
   The minimum follows an exponential distribution whose mean is $1/\left(\sum_{i=1}^{n}\frac{1}{\mu_{i}} \right)$.
\end{solution}
   \question
   Let $X_{1}, X_{2}, \ldots , X_{n}$ be a random sample from $\mathcal{N}\left(\mu, \sigma^{2}\right)$.\\
   \textbf{(1)} If $\mu, \sigma^{2}$ are unknown, explain the MLE method for $\mu, \sigma^{2}$.\\
   \textbf{(2)} Is the MLE for $\sigma^{2}$ the MVUE?
   \begin{solution}
      \textbf{(1)} The method of maximum likelihood attempts to find the optimal point in the domain that maximizes the likelihood function. According to optimization theory, we can find the optimal point of a function by differentiating it and equating it with 0 if the function is differentiable within the given domain. It is possible to take the derivative with respect to the parameter of interest directly to the function. However, for computational convenience, it is more commonplace to take the logarithm before differentiation on the ground that a logarithmic function is a monotically increasing function. Finally, to check whether the calculated optimum is a minimum or a maximum, we take the second derivative and see if it is positive or negative in the given region.\par
      This methodology is also applicable to a normal distribution with unknown mean and variance.
      \begin{align*}
         L\left(\mu, \sigma^{2}\right) &= \left(2\pi \sigma^{2} \right)^{-n/2}\exp \left\{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2} \right\}\\
         \ln L\left(\mu, \sigma^{2}\right) &= -\frac{n}{2}\ln \left(2\pi\sigma^{2}\right) - \frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}\\
         \frac{\partial}{\partial \mu}\ln L\left(\mu, \sigma^{2}\right) &= \frac{1}{\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right) = 0 \qquad \cdots (1)\\
         \frac{\partial}{\partial \sigma^{2}}\ln L\left(\mu, \sigma^{2}\right) &= -\frac{n}{2\sigma^{2}} +\frac{1}{2\left(\sigma^{2}\right)^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2} = 0 \qquad \cdots (2)
      \end{align*}
      From eqn (1), we can immediately find out that $\hat{\mu}^{\text{MLE}} = n^{-1}\sum_{i=1}^{n}X_{i}$, which is the sample mean. Further, from eqn (2), it is obvious that $\widehat{\sigma^{2}}^{\text{MLE}}=n^{-1}\sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}$. Since this is a two-dimensional parameter space, we can plug in the MLE for $\mu$ to the second result thereby yielding $n^{-1}\sum_{i=1}^{n}\left(X_{i} - \hat{\mu}^{\text{MLE}}\right)^{2}$. (We will not go through the second derivatives.)
   \end{solution}
   \begin{solution}
      \textbf{(2)} \emph{MVUE} stands for the \emph{minimum variance unbiased estimator}. It is a term that describes an estimator in the class of all unbiased estimators that have the smallest variance. Recall the \emph{bias-variance trade-off}. If we drop the `unbiasedness' requirement , then apparently, the variance becomes smaller; there is no point in comparing an unbiased estimator and a biased one due to this nature. It is a well-known fact that an uncorrected sample variance is biased with an expected value of $n^{-1}\left(n-1\right)\sigma^{2}$ regardless of what distribution each random variable follows. Let us write $\overline{X}_{n}$ for $\hat{\mu}^{\text{MLE}}$.
      \begin{align*}
         \mathbb{E}\left[\widehat{\sigma}^{2}\right] &= \mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\left(X_{i}-\overline{X}_{n}\right)^{2} \right]\\
         &= \frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[X_{i}^{2} - 2X_{i}\overline{X}_{n} + \overline{X}_{n}^{2}  \right]\\
         &= \frac{1}{n}\left(\sum_{i=1}^{n}\left(\mu^{2}+\sigma^{2}\right) - 2n\mathbb{E}\left[\overline{X}_{n}^{2}\right] + n\mathbb{E}\left[\overline{X}_{n}^{2}\right]\right)\\
         &= \frac{1}{n}\left(n\left(\mu^{2}+\sigma^{2}\right)-n\mathbb{E}\left[\overline{X}_{n}^{2}\right] \right)\\
         &= \frac{1}{n}\left(n\left(\mu^{2}+\sigma^{2}\right) - n\left(\mu^{2}+\frac{\sigma^{2}}{n}\right) \right)\\
         &= \frac{n-1}{n}\sigma^{2}
      \end{align*}
      Therefore, the MLE of $\sigma^{2}$ does not even count as a candidate to be compared its variance as it failed to become an unbiased estimator.
   \end{solution}
   \question
   Independent random variables $Y_{i}\,\left(i=1,2,\ldots , n\right)$ each follows $\mathcal{N}\left(\beta_{0}+\beta_{1}x_{i}, \sigma^{2}\right)$.\par
   \textbf{(1)} Derive the maximum likelihood estimator(MLE) of the parameter $\sigma^{2}$.\\
   \textbf{(2)} Show whether the estimator derived in \textbf{(1)} is biased or not.
   \begin{solution}
      \textbf{(1)} This is not very different from a linear regression setting. Let us first write down the likelihood function of $Y_{i}$.
      $$
         L\left(\beta_{0}, \beta_{1}, \sigma^{2}\right) = \left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2} \right\}
      $$
      As described in the previous answer, we will take the logarithm of the likelihood function and for simplicity, it will be denoted with $\ell\left(\beta_{0},\beta_{1},\sigma^{2}\right)$.
      \begin{align*}
         \ell\left(\beta_{0}, \beta_{1},\sigma^{2}\right) &= -\frac{n}{2}\ln\left(2\pi\sigma^{2}\right) -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2}\\
         \frac{\partial}{\partial \beta_{0}}\ell\left(\beta_{0},\beta_{1},\sigma^{2}\right) &= \frac{1}{\sigma^{2}}\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)=0 \qquad \cdots (1)\\
         \frac{\partial}{\partial \beta_{1}}\ell\left(\beta_{0},\beta_{1},\sigma^{2}\right) &= \frac{1}{\sigma^{2}}\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)x_{i}=0 \qquad \cdots (2)\\
         \frac{\partial}{\partial \sigma^{2}}\ell\left(\beta_{0},\beta_{1},\sigma^{2}\right) &= -\frac{n}{2\sigma^{2}} + \frac{1}{2\left(\sigma^{2}\right)^{2}}\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i}\right)^{2} =0 \qquad \cdots (3)
      \end{align*}
      Rearranging eqn \textbf{(1)} with respect to $\beta_{0}$,
      $$
         \hat{\beta_{0}} = \overline{y}_{n} - \hat{\beta_{1}}\overline{x}_{n}.
      $$
      Likewise with eqns \textbf{(2)}, \textbf{(3)},
      \begin{align*}
         \hat{\beta_{1}} &= \frac{\sum_{i=1}^{n}x_{i}y_{i}-\hat{\beta_{0}}\sum_{i=1}^{n}x_{i}}{\sum_{i=1}^{n}x_{i}^{2}}\\
         \widehat{\sigma}^{2} &= \frac{1}{n}\sum_{i=1}^{n}\left(y_{i}-\hat{\beta_{0}}-\hat{\beta_{1}}x_{i}\right)^{2}
      \end{align*}
      Since $\hat{\beta_{0}}, \hat{\beta_{1}}$ depend on each other, we can use the two equations which produces
      \begin{align*}
         \hat{\beta_{1}} &= \frac{\sum_{i=1}^{n}x_{i}y_{i} - \hat{\beta_{0}}\sum_{i=1}^{n}x_{i}}{\sum_{i=1}^{n}x_{i}^{2}}\\
         &= \frac{\sum_{i=1}^{n}x_{i}y_{i} - \left(\overline{y}_{n}-\hat{\beta_{1}}\overline{x}_{n} \right)\sum_{i=1}^{n}x_{i}}{\sum_{i=1}^{n}x_{i}^{2}}\\
         \left(1-\frac{\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}}{\sum_{i=1}^{n}x_{i}^{2}} \right)\hat{\beta_{1}} &= \frac{\sum_{i=1}^{n}x_{i}y_{i} - \overline{y}_{n}\sum_{i=1}^{n}x_{i}}{\sum_{i=1}^{n}x_{i}^{2}}\\
         \hat{\beta_{1}} &= \frac{\sum_{i=1}^{n}x_{i}y_{i} - \overline{y}_{n}\sum_{i=1}^{n}x_{i}}{\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}x_{i}\right)^{2}}\\
         &= \frac{\frac{1}{n}\sum_{i=1}^{n}x_{i}y_{i} - \left(\frac{1}{n}\sum_{i=1}^{n}x_{i}\right)\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}\right)}{\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}-\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}\right)^{2}}
      \end{align*}
      We have just rediscovered that the least square estimator and the maximum likelihood estimator are identical under a linear regression. With these estimators for $\beta_{0}, \beta_{1}$, the MLE of $\sigma^{2}$ is $n^{-1}\sum_{i=1}^{n}\left(y_{i}-\hat{\beta_{0}}-\hat{\beta_{1}}x_{i}\right)^{2}$.
   \end{solution}
   \begin{solution}
      \textbf{(2)} We all know the answer is a resounding \emph{NO!} but since it is a problem, we will try to mathematically prove that it is a biased estimator. For convenience, we will introduce a matrix notation. A random variable with no subscript will from now on denote a random vector. For example, $y = \begin{bmatrix} y_{1} & y_{2} & \cdots & y_{n}\end{bmatrix}^{T}$. Then
      \begin{align*}
         X &= \begin{bmatrix} 1  & x_{1} \\ 1 & x_{2}\\ \vdots & \vdots \\ 1 & x_{n} \end{bmatrix}\\
         \hat{\beta} &= \begin{bmatrix} \hat{\beta_{0}} & \hat{\beta_{1}} \end{bmatrix}^{T}\\
         \widehat{\sigma}^{2} &= \frac{1}{n}\left(y-X\hat{\beta}\right)^{T}\left(y-X\hat{\beta}\right)
      \end{align*}
      The estimator $\hat{\beta}$ has the relation $\hat{\beta} = \left(X^{T}X\right)^{-1}X^{T}y$. Furthermore, $y \sim \mathcal{N}\left(X\beta, \sigma^{2}I_{n}\right)$. Therefore,
      \begin{align*}
         \mathbb{E}\left[\hat{\beta}\right] &= \mathbb{E}\left[\left(X^{T}X\right)^{-1}X^{T}y\right]\\
         &= \left(X^{T}X\right)^{-1}X^{T}\mathbb{E}\left[y\right]\\
         &= \cancel{\left(X^{T}X\right)^{-1}}\cancel{X^{T}X}\beta\\
         &= \beta
      \end{align*}
      It is easy to show that the MLE for $\beta$ is an unbiased one. That being said,
      \begin{align*}
         \mathbb{E}\left[\widehat{\sigma}^{2} \right] &= \frac{1}{n}\mathbb{E}\left[\left(y-X\hat{\beta}\right)^{T}\left(y-X\hat{\beta}\right)\right].
      \end{align*}
      Let us write $H = X\left(X^{T}X\right)^{-1}X^{T}$ which is called the \emph{hat matrix} in linear regression. We also call this the \emph{projection matrix} and thus denote it by $P$ in linear algebra. It allows us to write $X\hat{\beta} = Hy$.
      \begin{align*}
         \mathbb{E}\left[\widehat{\sigma}^{2}\right] &= \frac{1}{n}\mathbb{E}\left[\left(y-Hy\right)^{T}\left(y-Hy\right)\right]\\
         &= \frac{1}{n}\mathbb{E}\left[y^{T}\left(I_{n}-H\right)^{T}\left(I_{n}-H\right)y\right]\\
         &= \frac{1}{n}\mathbb{E}\left[y^{T}\left(I_{n}-H\right)y\right] \quad \left(\because \left(I_{n}-H\right)=\left(I_{n}-H\right)^{T}=\left(I_{n}-H\right)^{2}\right)\\
         &= \frac{1}{n}\mathbb{E}\left[\Tr \left(I_{n}-H\right)yy^{T}\right] \quad \left(\because y^{T}\left(I_{n}-H\right)y \text{ is a scalar}.\right)\\
         &= \frac{1}{n}\Tr\left(\left(I_{n}-H\right)\mathbb{E}\left[yy^{T}\right]\right) \quad \left(\because \Tr, \mathbb{E} \text{ are linear}.\right)\\
         &= \frac{1}{n}\Tr \left(\left(I_{n}-H\right)\left(\sigma^{2}I_{n}+X\beta\beta^{T}X^{T}\right) \right)\\
         &= \frac{1}{n}\left(\Tr\left(\sigma^{2}I_{n}\right)+\Tr\left(X\beta\beta^{T}X^{T}\right) -\Tr\left(\sigma^{2}H\right)-\Tr\left(HX\beta\beta^{T}X^{T}\right) \right)
      \end{align*}
      $HX\beta\beta^{T}X^{T} = X\cancel{\left(X^{T}X\right)^{-1}}\cancel{X^{T}X}\beta\beta^{T}X^{T}=X\beta\beta^{T}X^{T}$. Therefore, the result is
      \begin{align*}
         \mathbb{E}\left[\widehat{\sigma}^{2}\right] &= \frac{1}{n}\left(\sigma^{2}\Tr\left(I_{n}\right) - \sigma^{2}\Tr\left(H\right) \right)\\
         &= \frac{1}{n}\left(n\sigma^{2} - \sigma^{2}\Tr\left(\left(X^{T}X\right)^{-1}X^{T}X\right)\right)\\
         &= \frac{1}{n}\left(n\sigma^{2}- \left(p+1\right)\sigma^{2}\right)\\
         &= \frac{n-p-1}{n}\sigma^{2}
      \end{align*}
      where $p$ is the number of predictor variables. In our case $p=1$. Thus, the expectation is $n^{-1}\left(n-2\right)\sigma^{2}$, which is biased.
   \end{solution}
      \question
      \textbf{(1)} The following table defines the p.m.f. of a random variable $X$ under a null hypothesis($\theta=\theta_{0}$) and an alternative hypothesis($\theta=\theta_{1}$). Compute the powers of the tests at $\alpha=0.05$ significance level and derive the \emph{most powerful test} via a comparison of the powers.
      \begin{table}[!htbp]
      \centering
        \begin{tabular}{*7c}
          \toprule
          $X$ & 1 & 2 & 3 & 4 & 5 & 6\\
          \midrule
          $f\left(x|\theta_{0}\right)$ & 0.01 & 0.02 & 0.02 & 0.05 & 0.10 & 0.80\\
          $f\left(x|\theta_{1}\right)$ & 0.03 & 0.05 & 0.05 & 0.10 & 0.10 & 0.67\\
          \bottomrule
        \end{tabular}
      \end{table}\\
      \textbf{(2)} Let $X_{1}, X_{2}, \ldots , X_{n}$ be a random sample from $\mathcal{N}\left(\mu,1\right)$.
      \begin{enumerate}[label=(\alph*)]
         \item If there exists a \emph{most powerful test} for a null hypothesis $H_{0}: \mu = 1$ and an alternative hypothesis $H_{1}:\mu=2$, derive it at 0.05 significance level and compute the power of the test used.
         \item If there exists a \emph{most powerful test} for a null hypothesis $H_{0}: \mu =1$ and an alternative hypothesis $H_{1}:\mu\neq 1$, derive it at 0.05 significance level. Explain if it does not exist.
         \item For the hypotheses given in the previous problem, compute the likelihood ratio test at 0.05 significance level and explain what optimal properties the method of testing has.
      \end{enumerate}
      \begin{solution}
         \textbf{(1)} 
      \end{solution}
\end{questions}
\end{document}
